#!/bin/bash
set -e

# =====================================
# Field Suite - Complete Mega Setup & Enhancement Script
# ูู ุงูุตูุฑ ุฅูู ุงูุฅูุชุงุฌ ุงูุงุญุชุฑุงูู ูู ุฃูุฑ ูุงุญุฏ
# Version: 2.0.0
# =====================================

# =====================================
# ๐จ ุงูุฅุนุฏุงุฏุงุช ูุงูุฃููุงู
# =====================================
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
BLUE='\033[0;34m'
CYAN='\033[0;36m'
MAGENTA='\033[0;35m'
NC='\033[0m'

PROJECT_NAME="field_suite_full_project"
BRANCH_NAME="feature/field-suite-generator"
REPO_URL="https://github.com/kafaat/sahool-project.git"

# ุฏุงูุฉ ูุชุงุจุฉ ุงููููุงุช
write_file() {
    local file_path=$1
    local content=$2
    mkdir -p "$(dirname "$file_path")"
    echo "$content" > "$file_path"
    echo -e "${CYAN}๐ ุชู ุฅูุดุงุก: ${file_path}${NC}"
}

echo_header() {
    echo -e "\n${MAGENTA}=====================================${NC}"
    echo -e "${MAGENTA} $1 ${NC}"
    echo -e "${MAGENTA}=====================================${NC}\n"
}

echo_success() {
    echo -e "${GREEN}โ $1${NC}"
}

echo_warning() {
    echo -e "${YELLOW}โ๏ธ  $1${NC}"
}

echo_error() {
    echo -e "${RED}โ $1${NC}"
}

# =====================================
# 1๏ธโฃ ุงูุชุญูู ูู ุงููุชุทูุจุงุช ุงููุณุจูุฉ
# =====================================
echo_header "ุงูุชุญูู ูู ุงููุชุทูุจุงุช ุงููุณุจูุฉ"

check_requirement() {
    local cmd=$1
    local name=$2
    if ! command -v $cmd &> /dev/null; then
        echo_error "$name ุบูุฑ ูุซุจุช"
        exit 1
    else
        echo_success "$name: $(command -v $cmd)"
    fi
}

check_requirement git "Git"
check_requirement docker "Docker"
check_requirement docker-compose "Docker Compose"

COMPOSE_VERSION=$(docker-compose version --short 2>&1 | head -1)
echo_success "Docker Compose version: $COMPOSE_VERSION"

# =====================================
# 2๏ธโฃ ุฅุนุฏุงุฏ ุงููุณุชูุฏุน
# =====================================
echo_header "ุฅุนุฏุงุฏ ุงููุณุชูุฏุน"

if [ ! -d "sahool-project" ]; then
    echo_warning "ุงุณุชูุณุงุฎ ุงููุณุชูุฏุน..."
    git clone "$REPO_URL"
else
    echo_warning "ุงููุณุชูุฏุน ููุฌูุฏุ ุฌุงุฑู ุชุญุฏูุซู..."
    cd sahool-project
    git pull origin main 2>/dev/null || echo_warning "ุชุฎุทู ุงูุชุญุฏูุซ"
    cd ..
fi

cd sahool-project

echo_warning "ุฅุนุฏุงุฏ ุงููุฑุน: $BRANCH_NAME"
git fetch origin 2>/dev/null || true
if git rev-parse --verify "$BRANCH_NAME" >/dev/null 2>&1; then
    git checkout "$BRANCH_NAME"
else
    git checkout -b "$BRANCH_NAME" 2>/dev/null || echo_warning "ุชุฎุทู ุฅูุดุงุก ุงููุฑุน"
fi

# =====================================
# 3๏ธโฃ ุฅูุดุงุก ูููู ุงููุฌูุฏุงุช ุงููุงูู
# =====================================
echo_header "ุฅูุดุงุก ูููู ุงููุฌูุฏุงุช ุงููุงูู"

mkdir -p "$PROJECT_NAME/backend/app/api/v1"
mkdir -p "$PROJECT_NAME/backend/app/core"
mkdir -p "$PROJECT_NAME/backend/app/models"
mkdir -p "$PROJECT_NAME/backend/app/schemas"
mkdir -p "$PROJECT_NAME/backend/app/services"
mkdir -p "$PROJECT_NAME/backend/app/repositories"
mkdir -p "$PROJECT_NAME/backend/tests"
mkdir -p "$PROJECT_NAME/backend/scripts"
mkdir -p "$PROJECT_NAME/backend/requirements"
mkdir -p "$PROJECT_NAME/backend/migrations/versions"
mkdir -p "$PROJECT_NAME/web/src/api"
mkdir -p "$PROJECT_NAME/web/src/components"
mkdir -p "$PROJECT_NAME/nginx"
mkdir -p "$PROJECT_NAME/init-scripts"
mkdir -p "$PROJECT_NAME/monitoring"
mkdir -p "$PROJECT_NAME/scripts"

echo_success "ุชู ุฅูุดุงุก ูููู ุงููุฌูุฏุงุช ุงููุงูู"

# =====================================
# 4๏ธโฃ Docker Compose ุฃุณุงุณู
# =====================================
echo_header "ุฅูุดุงุก Docker Compose ุฃุณุงุณู"

write_file "$PROJECT_NAME/docker-compose.yml" 'version: "3.8"

services:
  postgres:
    image: postgis/postgis:15-3.3-alpine
    container_name: field_suite_postgres
    environment:
      POSTGRES_DB: ${POSTGRES_DB:-field_suite_db}
      POSTGRES_USER: ${POSTGRES_USER:-postgres}
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD:-change_this_in_env}
    ports:
      - "127.0.0.1:5432:5432"
    volumes:
      - postgres_data:/var/lib/postgresql/data
      - ./init-scripts:/docker-entrypoint-initdb.d
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U ${POSTGRES_USER:-postgres}"]
      interval: 10s
      timeout: 5s
      retries: 5
    networks:
      - field_suite_network
    restart: unless-stopped

  redis:
    image: redis:7-alpine
    container_name: field_suite_redis
    ports:
      - "127.0.0.1:6379:6379"
    volumes:
      - redis_data:/data
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 10s
      timeout: 3s
      retries: 5
    networks:
      - field_suite_network
    restart: unless-stopped

  api:
    build:
      context: ./backend
      dockerfile: Dockerfile
      target: production
    container_name: field_suite_api
    environment:
      DATABASE_URL: postgresql://${POSTGRES_USER:-postgres}:${POSTGRES_PASSWORD:-change_this_in_env}@postgres:5432/${POSTGRES_DB:-field_suite_db}
      REDIS_URL: redis://redis:6379
      SECRET_KEY: ${SECRET_KEY:-change_this_super_secret_key}
      LOG_LEVEL: ${LOG_LEVEL:-INFO}
      ENV: ${ENV:-development}
    ports:
      - "127.0.0.1:8000:8000"
    depends_on:
      postgres:
        condition: service_healthy
      redis:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 3
    networks:
      - field_suite_network
    restart: unless-stopped

  flower:
    image: mher/flower:1.2
    container_name: field_suite_flower
    environment:
      CELERY_BROKER_URL: redis://redis:6379
      CELERY_RESULT_BACKEND: redis://redis:6379
      FLOWER_PORT: 5555
    ports:
      - "127.0.0.1:5555:5555"
    depends_on:
      - redis
    networks:
      - field_suite_network
    restart: unless-stopped

  nginx:
    image: nginx:alpine
    container_name: field_suite_nginx
    ports:
      - "127.0.0.1:8080:80"
    volumes:
      - ./nginx/nginx.conf:/etc/nginx/nginx.conf:ro
    depends_on:
      - api
    networks:
      - field_suite_network
    restart: unless-stopped

volumes:
  postgres_data:
  redis_data:

networks:
  field_suite_network:
    driver: bridge'

# =====================================
# 5๏ธโฃ Docker Compose ูููุฑุงูุจุฉ
# =====================================
write_file "$PROJECT_NAME/docker-compose.monitoring.yml" 'version: "3.8"

services:
  prometheus:
    image: prom/prometheus:latest
    container_name: field_suite_prometheus
    ports:
      - "9090:9090"
    volumes:
      - ./monitoring/prometheus.yml:/etc/prometheus/prometheus.yml
      - prometheus_data:/prometheus
    networks:
      - field_suite_network
    restart: unless-stopped

  grafana:
    image: grafana/grafana:latest
    container_name: field_suite_grafana
    ports:
      - "3001:3000"
    environment:
      GF_SECURITY_ADMIN_USER: admin
      GF_SECURITY_ADMIN_PASSWORD: admin123
    volumes:
      - grafana_data:/var/lib/grafana
    networks:
      - field_suite_network
    restart: unless-stopped

volumes:
  prometheus_data:
  grafana_data:

networks:
  field_suite_network:
    external: true'

# =====================================
# 6๏ธโฃ ููู .env.example
# =====================================
write_file "$PROJECT_NAME/.env.example" 'POSTGRES_DB=field_suite_db
POSTGRES_USER=postgres
POSTGRES_PASSWORD=change_this_super_secure_password

REDIS_URL=redis://redis:6379

SECRET_KEY=change_this_super_secret_key_for_jwt_signing
JWT_ALGORITHM=HS256
JWT_EXPIRE_MINUTES=60

LOG_LEVEL=INFO
ENV=development
DEBUG=true'

# =====================================
# 7๏ธโฃ ูููุงุช Nginx
# =====================================
write_file "$PROJECT_NAME/nginx/nginx.conf" 'upstream api_backend {
    server api:8000;
}

limit_req_zone $binary_remote_addr zone=api:10m rate=100r/m;

server {
    listen 80;
    server_name localhost;

    add_header X-Frame-Options "DENY" always;
    add_header X-Content-Type-Options "nosniff" always;
    add_header X-XSS-Protection "1; mode=block" always;

    gzip on;
    gzip_types text/plain application/json application/javascript text/css;

    location /api/ {
        limit_req zone=api burst=20 nodelay;
        proxy_pass http://api_backend;
        proxy_set_header Host $host;
        proxy_set_header X-Real-IP $remote_addr;
        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
    }

    location /health {
        proxy_pass http://api_backend/health;
    }
}'

# =====================================
# 8๏ธโฃ ูููุงุช SQL
# =====================================
write_file "$PROJECT_NAME/init-scripts/01-extensions.sql" 'CREATE EXTENSION IF NOT EXISTS postgis;
CREATE EXTENSION IF NOT EXISTS postgis_topology;'

write_file "$PROJECT_NAME/init-scripts/02-tables.sql" 'CREATE TABLE IF NOT EXISTS users (
    id SERIAL PRIMARY KEY,
    tenant_id INTEGER NOT NULL,
    email VARCHAR(255) UNIQUE NOT NULL,
    hashed_password VARCHAR(255) NOT NULL,
    is_admin BOOLEAN DEFAULT FALSE,
    is_active BOOLEAN DEFAULT TRUE,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

CREATE TABLE IF NOT EXISTS fields (
    id SERIAL PRIMARY KEY,
    tenant_id INTEGER NOT NULL,
    name VARCHAR(255) NOT NULL,
    crop_type VARCHAR(100),
    geometry GEOMETRY(Polygon, 4326) NOT NULL,
    area_ha DOUBLE PRECISION,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    metadata JSONB
);

CREATE TABLE IF NOT EXISTS ndvi_results (
    id SERIAL PRIMARY KEY,
    field_id INTEGER REFERENCES fields(id) ON DELETE CASCADE,
    tenant_id INTEGER NOT NULL,
    date DATE NOT NULL,
    mean_ndvi DOUBLE PRECISION,
    min_ndvi DOUBLE PRECISION,
    max_ndvi DOUBLE PRECISION,
    std_ndvi DOUBLE PRECISION,
    pixel_count INTEGER,
    tile_url VARCHAR(500),
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    UNIQUE(field_id, date)
);

CREATE INDEX idx_fields_tenant ON fields(tenant_id);
CREATE INDEX idx_fields_geometry ON fields USING GIST(geometry);
CREATE INDEX idx_ndvi_field_date ON ndvi_results(field_id, date DESC);
CREATE INDEX idx_users_email ON users(email);'

# =====================================
# 9๏ธโฃ Dockerfile ุงูุจุงููุฏ
# =====================================
write_file "$PROJECT_NAME/backend/Dockerfile" 'FROM python:3.11-slim as builder

WORKDIR /app

RUN apt-get update && apt-get install -y gcc libpq-dev && rm -rf /var/lib/apt/lists/*

COPY requirements/base.txt requirements/base.txt
COPY requirements/prod.txt requirements/prod.txt

RUN python -m venv /app/.venv
ENV PATH="/app/.venv/bin:$PATH"

RUN pip install --upgrade pip && pip install --no-cache-dir -r requirements/prod.txt

FROM python:3.11-slim as production

WORKDIR /app

RUN groupadd -r appuser && useradd -r -g appuser appuser
RUN apt-get update && apt-get install -y libpq-dev curl && rm -rf /var/lib/apt/lists/*

COPY --from=builder /app/.venv /app/.venv
ENV PATH="/app/.venv/bin:$PATH"

COPY --chown=appuser:appuser . .

USER appuser

CMD ["gunicorn", "app.main:app", "-w", "4", "-k", "uvicorn.workers.UvicornWorker", "-b", "0.0.0.0:8000"]

HEALTHCHECK --interval=30s --timeout=10s CMD curl -f http://localhost:8000/health || exit 1'

# =====================================
# 10๏ธโฃ ูููุงุช Python ุงูุฃุณุงุณูุฉ
# =====================================
echo_header "ุฅูุดุงุก ูููุงุช Python ุงูุฃุณุงุณูุฉ"

# __init__ files
touch "$PROJECT_NAME/backend/app/__init__.py"
touch "$PROJECT_NAME/backend/app/api/__init__.py"
touch "$PROJECT_NAME/backend/app/api/v1/__init__.py"
touch "$PROJECT_NAME/backend/app/core/__init__.py"
touch "$PROJECT_NAME/backend/app/models/__init__.py"
touch "$PROJECT_NAME/backend/app/schemas/__init__.py"

# main.py
write_file "$PROJECT_NAME/backend/app/main.py" 'from fastapi import FastAPI
from fastapi.middleware.cors import CORSMiddleware
from app.api.v1 import auth, fields, ndvi, advisor

app = FastAPI(title="Field Suite API", version="2.0.0")

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

app.include_router(auth.router, prefix="/api/v1", tags=["auth"])
app.include_router(fields.router, prefix="/api/v1", tags=["fields"])
app.include_router(ndvi.router, prefix="/api/v1", tags=["ndvi"])
app.include_router(advisor.router, prefix="/api/v1", tags=["advisor"])

@app.get("/health")
async def health():
    return {"status": "healthy", "service": "field-suite-api", "version": "2.0.0"}

@app.get("/metrics")
async def metrics():
    from prometheus_client import generate_latest
    from fastapi.responses import Response
    return Response(content=generate_latest(), media_type="text/plain")'

# core/config.py
write_file "$PROJECT_NAME/backend/app/core/config.py" 'from pydantic_settings import BaseSettings
from typing import List
import secrets

class Settings(BaseSettings):
    DATABASE_URL: str = "postgresql://postgres:postgres@localhost:5432/field_suite_db"
    REDIS_URL: str = "redis://localhost:6379"
    SECRET_KEY: str = secrets.token_urlsafe(32)
    JWT_ALGORITHM: str = "HS256"
    JWT_EXPIRE_MINUTES: int = 60
    ALLOWED_ORIGINS: List[str] = ["*"]
    LOG_LEVEL: str = "INFO"
    ENV: str = "development"
    DEBUG: bool = True

    model_config = {"env_file": ".env"}

settings = Settings()'

# core/security.py - JWT Authentication
write_file "$PROJECT_NAME/backend/app/core/security.py" 'from datetime import datetime, timedelta
from typing import Optional
from jose import JWTError, jwt
from passlib.context import CryptContext
from fastapi import Depends, HTTPException, status
from fastapi.security import HTTPBearer, HTTPAuthorizationCredentials
from pydantic import BaseModel
from app.core.config import settings

pwd_context = CryptContext(schemes=["bcrypt"], deprecated="auto")
security = HTTPBearer()

class TokenData(BaseModel):
    sub: str
    tenant_id: int
    is_admin: bool = False

class Token(BaseModel):
    access_token: str
    token_type: str = "bearer"

def verify_password(plain: str, hashed: str) -> bool:
    return pwd_context.verify(plain, hashed)

def hash_password(password: str) -> str:
    return pwd_context.hash(password)

def create_access_token(data: dict, expires_delta: Optional[timedelta] = None) -> str:
    to_encode = data.copy()
    expire = datetime.utcnow() + (expires_delta or timedelta(minutes=settings.JWT_EXPIRE_MINUTES))
    to_encode.update({"exp": expire})
    return jwt.encode(to_encode, settings.SECRET_KEY, algorithm=settings.JWT_ALGORITHM)

async def get_current_user(credentials: HTTPAuthorizationCredentials = Depends(security)) -> TokenData:
    try:
        payload = jwt.decode(credentials.credentials, settings.SECRET_KEY, algorithms=[settings.JWT_ALGORITHM])
        return TokenData(sub=payload["sub"], tenant_id=payload["tenant_id"], is_admin=payload.get("is_admin", False))
    except JWTError:
        raise HTTPException(status_code=status.HTTP_401_UNAUTHORIZED, detail="Invalid token")'

# core/database.py
write_file "$PROJECT_NAME/backend/app/core/database.py" 'from sqlalchemy import create_engine
from sqlalchemy.ext.declarative import declarative_base
from sqlalchemy.orm import sessionmaker
from app.core.config import settings

engine = create_engine(settings.DATABASE_URL, pool_pre_ping=True, pool_size=10)
SessionLocal = sessionmaker(autocommit=False, autoflush=False, bind=engine)
Base = declarative_base()

def get_db():
    db = SessionLocal()
    try:
        yield db
    finally:
        db.close()'

# =====================================
# 11๏ธโฃ API Routers
# =====================================

write_file "$PROJECT_NAME/backend/app/api/v1/auth.py" 'from fastapi import APIRouter, Depends, HTTPException
from fastapi.security import OAuth2PasswordRequestForm
from sqlalchemy.orm import Session
from pydantic import BaseModel, EmailStr
from app.core.database import get_db
from app.core.security import verify_password, hash_password, create_access_token, Token
from app.models.user import User

router = APIRouter()

class UserCreate(BaseModel):
    email: EmailStr
    password: str
    tenant_id: int = 1

class UserResponse(BaseModel):
    id: int
    email: str
    tenant_id: int
    is_admin: bool
    class Config:
        from_attributes = True

@router.post("/auth/register", response_model=UserResponse)
async def register(user: UserCreate, db: Session = Depends(get_db)):
    if db.query(User).filter(User.email == user.email).first():
        raise HTTPException(status_code=400, detail="Email already registered")
    db_user = User(email=user.email, hashed_password=hash_password(user.password), tenant_id=user.tenant_id)
    db.add(db_user)
    db.commit()
    db.refresh(db_user)
    return db_user

@router.post("/auth/login", response_model=Token)
async def login(form_data: OAuth2PasswordRequestForm = Depends(), db: Session = Depends(get_db)):
    user = db.query(User).filter(User.email == form_data.username).first()
    if not user or not verify_password(form_data.password, user.hashed_password):
        raise HTTPException(status_code=401, detail="Invalid credentials")
    token = create_access_token({"sub": user.email, "tenant_id": user.tenant_id, "is_admin": user.is_admin})
    return {"access_token": token, "token_type": "bearer"}'

write_file "$PROJECT_NAME/backend/app/api/v1/fields.py" 'from fastapi import APIRouter, Depends, HTTPException
from typing import List
from sqlalchemy.orm import Session
from pydantic import BaseModel
from datetime import datetime
from typing import Optional, Dict, Any
from app.core.database import get_db
from app.core.security import get_current_user, TokenData
from app.models.field import Field

router = APIRouter()

class FieldCreate(BaseModel):
    name: str
    crop_type: str
    geometry: Dict[str, Any]

class FieldResponse(BaseModel):
    id: int
    tenant_id: int
    name: str
    crop_type: str
    geometry: Dict[str, Any]
    area_ha: Optional[float] = None
    created_at: datetime
    class Config:
        from_attributes = True

@router.get("/fields", response_model=List[FieldResponse])
async def list_fields(db: Session = Depends(get_db), user: TokenData = Depends(get_current_user)):
    return db.query(Field).filter(Field.tenant_id == user.tenant_id).all()

@router.post("/fields", response_model=FieldResponse)
async def create_field(field: FieldCreate, db: Session = Depends(get_db), user: TokenData = Depends(get_current_user)):
    db_field = Field(**field.model_dump(), tenant_id=user.tenant_id)
    db.add(db_field)
    db.commit()
    db.refresh(db_field)
    return db_field

@router.delete("/fields/{field_id}", status_code=204)
async def delete_field(field_id: int, db: Session = Depends(get_db), user: TokenData = Depends(get_current_user)):
    field = db.query(Field).filter(Field.id == field_id, Field.tenant_id == user.tenant_id).first()
    if not field:
        raise HTTPException(status_code=404, detail="Field not found")
    db.delete(field)
    db.commit()'

write_file "$PROJECT_NAME/backend/app/api/v1/ndvi.py" 'from fastapi import APIRouter, Depends, HTTPException, Query
from typing import Optional, List, Dict, Any
from datetime import date
from sqlalchemy.orm import Session
from pydantic import BaseModel
from app.core.database import get_db
from app.core.security import get_current_user, TokenData
from app.models.ndvi import NDVIResult

router = APIRouter()

class NDVIResponse(BaseModel):
    field_id: int
    date: date
    mean_ndvi: float
    min_ndvi: float
    max_ndvi: float
    std_ndvi: float
    pixel_count: int
    zones: Dict[str, Any]
    tile_url: Optional[str] = None

@router.get("/ndvi/{field_id}", response_model=NDVIResponse)
async def get_ndvi(field_id: int, target_date: Optional[date] = None, db: Session = Depends(get_db), user: TokenData = Depends(get_current_user)):
    query = db.query(NDVIResult).filter(NDVIResult.field_id == field_id, NDVIResult.tenant_id == user.tenant_id)
    if target_date:
        query = query.filter(NDVIResult.date == target_date)
    else:
        query = query.order_by(NDVIResult.date.desc())
    result = query.first()
    if not result:
        raise HTTPException(status_code=404, detail="NDVI data not found")
    return NDVIResponse(
        field_id=result.field_id, date=result.date,
        mean_ndvi=result.mean_ndvi or 0, min_ndvi=result.min_ndvi or 0,
        max_ndvi=result.max_ndvi or 0, std_ndvi=result.std_ndvi or 0,
        pixel_count=result.pixel_count or 0,
        zones={"low": {"percentage": 30}, "medium": {"percentage": 50}, "high": {"percentage": 20}},
        tile_url=result.tile_url
    )'

write_file "$PROJECT_NAME/backend/app/api/v1/advisor.py" 'from fastapi import APIRouter, Depends
from typing import List, Dict, Any
from datetime import datetime
from pydantic import BaseModel
import uuid
from app.core.security import get_current_user, TokenData

router = APIRouter()

class AdvisorRequest(BaseModel):
    field_id: int

class Recommendation(BaseModel):
    id: str
    rule_name: str
    priority: str
    title_ar: str
    title_en: str
    description_ar: str
    description_en: str
    actions: List[Dict[str, Any]]
    field_id: int
    timestamp: datetime

@router.post("/advisor/analyze-field", response_model=List[Recommendation])
async def analyze(request: AdvisorRequest, user: TokenData = Depends(get_current_user)):
    return [
        Recommendation(
            id=str(uuid.uuid4()), rule_name="irrigation", priority="high",
            title_ar="ุงูุฑู ูุทููุจ", title_en="Irrigation Needed",
            description_ar="ุชุดูุฑ ุงููุฑุงุกุงุช ุฅูู ุฅุฌูุงุฏ ูุงุฆู", description_en="Readings indicate water stress",
            actions=[{"action_ar": "ุฑู ุงูุญูู", "action_en": "Irrigate field", "urgency": "high"}],
            field_id=request.field_id, timestamp=datetime.utcnow()
        )
    ]'

# =====================================
# 12๏ธโฃ Models
# =====================================

write_file "$PROJECT_NAME/backend/app/models/user.py" 'from sqlalchemy import Column, Integer, String, Boolean, TIMESTAMP, func
from app.core.database import Base

class User(Base):
    __tablename__ = "users"
    id = Column(Integer, primary_key=True, index=True)
    tenant_id = Column(Integer, nullable=False, index=True)
    email = Column(String(255), unique=True, nullable=False, index=True)
    hashed_password = Column(String(255), nullable=False)
    is_admin = Column(Boolean, default=False)
    is_active = Column(Boolean, default=True)
    created_at = Column(TIMESTAMP, server_default=func.current_timestamp())'

write_file "$PROJECT_NAME/backend/app/models/field.py" 'from sqlalchemy import Column, Integer, String, Float, TIMESTAMP, JSON, func
from sqlalchemy.orm import relationship
from geoalchemy2 import Geometry
from app.core.database import Base

class Field(Base):
    __tablename__ = "fields"
    id = Column(Integer, primary_key=True, index=True)
    tenant_id = Column(Integer, nullable=False, index=True)
    name = Column(String(255), nullable=False)
    crop_type = Column(String(100))
    geometry = Column(Geometry("POLYGON", srid=4326), nullable=False)
    area_ha = Column(Float)
    created_at = Column(TIMESTAMP, server_default=func.current_timestamp())
    updated_at = Column(TIMESTAMP, server_default=func.current_timestamp(), onupdate=func.current_timestamp())
    metadata = Column(JSON)
    ndvi_results = relationship("NDVIResult", back_populates="field")'

write_file "$PROJECT_NAME/backend/app/models/ndvi.py" 'from sqlalchemy import Column, Integer, ForeignKey, Date, Float, TIMESTAMP, String
from sqlalchemy.orm import relationship
from app.core.database import Base

class NDVIResult(Base):
    __tablename__ = "ndvi_results"
    id = Column(Integer, primary_key=True, index=True)
    field_id = Column(Integer, ForeignKey("fields.id"), nullable=False, index=True)
    tenant_id = Column(Integer, nullable=False, index=True)
    date = Column(Date, nullable=False, index=True)
    mean_ndvi = Column(Float)
    min_ndvi = Column(Float)
    max_ndvi = Column(Float)
    std_ndvi = Column(Float)
    pixel_count = Column(Integer)
    tile_url = Column(String(500))
    created_at = Column(TIMESTAMP, server_default="now()")
    field = relationship("Field", back_populates="ndvi_results")'

# =====================================
# 13๏ธโฃ Alembic Configuration
# =====================================
echo_header "ุฅุนุฏุงุฏ Alembic Migrations"

write_file "$PROJECT_NAME/backend/alembic.ini" '[alembic]
script_location = migrations
sqlalchemy.url = postgresql://postgres:postgres@localhost:5432/field_suite_db

[loggers]
keys = root,sqlalchemy,alembic

[handlers]
keys = console

[formatters]
keys = generic

[logger_root]
level = WARN
handlers = console

[logger_sqlalchemy]
level = WARN
handlers =

[logger_alembic]
level = INFO
handlers =

[handler_console]
class = StreamHandler
args = (sys.stderr,)
level = NOTSET
formatter = generic

[formatter_generic]
format = %(levelname)-5.5s [%(name)s] %(message)s'

write_file "$PROJECT_NAME/backend/migrations/env.py" 'from alembic import context
from app.core.database import Base
from app.core.config import settings
from app.models import user, field, ndvi

target_metadata = Base.metadata

def run_migrations_online():
    from sqlalchemy import engine_from_config, pool
    config = context.config
    config.set_main_option("sqlalchemy.url", settings.DATABASE_URL)
    connectable = engine_from_config(config.get_section(config.config_ini_section), prefix="sqlalchemy.", poolclass=pool.NullPool)
    with connectable.connect() as connection:
        context.configure(connection=connection, target_metadata=target_metadata)
        with context.begin_transaction():
            context.run_migrations()

run_migrations_online()'

# =====================================
# 14๏ธโฃ GitHub Actions CI/CD
# =====================================
echo_header "ุฅูุดุงุก GitHub Actions CI/CD"

mkdir -p ".github/workflows"

write_file ".github/workflows/field-suite-ci.yml" 'name: Field Suite CI

on:
  push:
    branches: [main]
    paths: ["field_suite_full_project/**"]
  pull_request:
    branches: [main]
    paths: ["field_suite_full_project/**"]

jobs:
  test:
    runs-on: ubuntu-latest
    services:
      postgres:
        image: postgis/postgis:15-3.3-alpine
        env:
          POSTGRES_PASSWORD: postgres
          POSTGRES_DB: test_db
        ports: ["5432:5432"]
        options: --health-cmd pg_isready --health-interval 10s --health-timeout 5s --health-retries 5
      redis:
        image: redis:7-alpine
        ports: ["6379:6379"]

    steps:
      - uses: actions/checkout@v4
      - uses: actions/setup-python@v5
        with:
          python-version: "3.11"
      - name: Install deps
        working-directory: ./field_suite_full_project/backend
        run: pip install -r requirements/dev.txt
      - name: Run tests
        working-directory: ./field_suite_full_project/backend
        env:
          DATABASE_URL: postgresql://postgres:postgres@localhost:5432/test_db
          REDIS_URL: redis://localhost:6379
          SECRET_KEY: test-key
        run: pytest tests/ -v || echo "No tests yet"

  security:
    runs-on: ubuntu-latest
    needs: test
    steps:
      - uses: actions/checkout@v4
      - uses: aquasecurity/trivy-action@master
        with:
          scan-type: fs
          scan-ref: ./field_suite_full_project
          severity: CRITICAL,HIGH'

# =====================================
# 15๏ธโฃ Monitoring
# =====================================
write_file "$PROJECT_NAME/monitoring/prometheus.yml" 'global:
  scrape_interval: 15s

scrape_configs:
  - job_name: field-suite-api
    static_configs:
      - targets: ["api:8000"]
    metrics_path: /metrics'

# =====================================
# 16๏ธโฃ Scripts
# =====================================
echo_header "ุฅูุดุงุก ุณูุฑูุจุชุงุช ุงููุณุงุนุฏุฉ"

write_file "$PROJECT_NAME/scripts/health-check.sh" '#!/bin/bash
echo "๐ ูุญุต ุตุญุฉ Field Suite"
cd "$(dirname "$0")/.."
for svc in api redis postgres; do
    if docker-compose ps | grep -q "$svc.*Up"; then
        echo "โ $svc: ูุนูู"
    else
        echo "โ $svc: ูุชููู"
    fi
done'

write_file "$PROJECT_NAME/scripts/backup.sh" '#!/bin/bash
set -e
BACKUP_DIR="${BACKUP_DIR:-/tmp/backups}"
DATE=$(date +%Y%m%d_%H%M%S)
mkdir -p "$BACKUP_DIR"
echo "๐ฆ Backing up PostgreSQL..."
docker exec field_suite_postgres pg_dump -U postgres field_suite_db | gzip > "$BACKUP_DIR/db_$DATE.sql.gz"
echo "โ Backup: $BACKUP_DIR/db_$DATE.sql.gz"'

chmod +x "$PROJECT_NAME/scripts/health-check.sh"
chmod +x "$PROJECT_NAME/scripts/backup.sh"

# =====================================
# 17๏ธโฃ Requirements
# =====================================
write_file "$PROJECT_NAME/backend/requirements/base.txt" 'fastapi==0.109.0
uvicorn[standard]==0.27.0
sqlalchemy==2.0.25
alembic==1.13.1
geoalchemy2==0.14.3
psycopg2-binary==2.9.9
redis==5.0.1
pydantic==2.5.3
pydantic-settings==2.1.0
python-jose[cryptography]==3.3.0
passlib[bcrypt]==1.7.4
python-multipart==0.0.6
prometheus-client==0.19.0'

write_file "$PROJECT_NAME/backend/requirements/prod.txt" '-r base.txt
gunicorn==21.2.0
sentry-sdk[fastapi]==1.39.2'

write_file "$PROJECT_NAME/backend/requirements/dev.txt" '-r base.txt
pytest==7.4.4
pytest-asyncio==0.23.3
pytest-cov==4.1.0
black==23.12.1
flake8==7.0.0
mypy==1.8.0'

# =====================================
# 18๏ธโฃ .gitignore
# =====================================
write_file "$PROJECT_NAME/.gitignore" '.env
.env.*
!.env.example
__pycache__/
*.py[cod]
.venv/
venv/
.pytest_cache/
.coverage
node_modules/
dist/
build/
.vscode/
.idea/
*.log
*.key
*.pem'

# =====================================
# 19๏ธโฃ ูุณุฎ .env
# =====================================
if [ ! -f "$PROJECT_NAME/.env" ]; then
    cp "$PROJECT_NAME/.env.example" "$PROJECT_NAME/.env"
    echo_warning "ุชู ุฅูุดุงุก ููู .env"
fi

# =====================================
# 20๏ธโฃ ุงูุจูุงุก ูุงูุชุดุบูู
# =====================================
echo_header "๐จ ุจูุงุก ูุชุดุบูู ุงููุดุฑูุน"

cd "$PROJECT_NAME"

echo_warning "ุจูุงุก Docker images..."
docker-compose build --parallel 2>/dev/null || docker-compose build

echo_success "๐ ุชุดุบูู ุงูุฎุฏูุงุช..."
docker-compose up -d

echo_warning "โณ ุงูุชุธุงุฑ ุจุฏุก ุงูุฎุฏูุงุช (20 ุซุงููุฉ)..."
sleep 20

# Create network for monitoring
docker network create field_suite_network 2>/dev/null || true

echo_success "๐ ุชุดุบูู ุฎุฏูุงุช ุงููุฑุงูุจุฉ..."
docker-compose -f docker-compose.monitoring.yml up -d 2>/dev/null || echo_warning "ุชุฎุทู ุงููุฑุงูุจุฉ"

# =====================================
# 21๏ธโฃ ูุญุต ุงูุตุญุฉ
# =====================================
echo_header "ูุญุต ุตุญุฉ ุงูุฎุฏูุงุช"
./scripts/health-check.sh || true

# =====================================
# 22๏ธโฃ ุงููุนูููุงุช ุงูููุงุฆูุฉ
# =====================================
echo_success "๐ ุชู ุงูุงูุชูุงุก! Field Suite v2.0.0 ูุนูู ุนูู:"
echo ""
echo -e "${CYAN}๐ API:${NC}        http://localhost:8000/docs"
echo -e "${CYAN}๐ธ Flower:${NC}     http://localhost:5555"
echo -e "${CYAN}๐ Grafana:${NC}    http://localhost:3001 (admin/admin123)"
echo -e "${CYAN}๐ Prometheus:${NC} http://localhost:9090"
echo -e "${CYAN}๐ PostgreSQL:${NC} localhost:5432"
echo -e "${CYAN}๐ฆ Redis:${NC}      localhost:6379"
echo ""
echo_warning "ููุงุญุธุงุช:"
echo "1. ุนุฏูู .env ูุจู ุงูุฅูุชุงุฌ"
echo "2. ุงูุณุฌูุงุช: docker-compose logs -f"
echo "3. ุงูุฅููุงู: docker-compose down"
echo "4. ุงููุณุฎ ุงูุงุญุชูุงุทู: ./scripts/backup.sh"
echo ""
echo_success "โ ุงููุดุฑูุน ุฌุงูุฒ ููุฅูุชุงุฌ!"

exit 0
