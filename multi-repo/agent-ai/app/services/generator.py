"""
Response Generation Module
Handles LLM invocation and response generation
"""

import os
import logging
from typing import Dict, Any, Optional

from langchain.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnablePassthrough, RunnableLambda

# LLM imports
try:
    from langchain_openai import ChatOpenAI
    OPENAI_AVAILABLE = True
except ImportError:
    OPENAI_AVAILABLE = False

try:
    from langchain_anthropic import ChatAnthropic
    ANTHROPIC_AVAILABLE = True
except ImportError:
    ANTHROPIC_AVAILABLE = False

logger = logging.getLogger(__name__)


class ResponseGenerator:
    """Generates responses using LLM or rule-based fallback"""

    def __init__(self, llm_provider: str = "openai", model_name: Optional[str] = None):
        """
        Initialize response generator

        Args:
            llm_provider: "openai", "anthropic", or "fallback"
            model_name: Specific model name
        """
        self.llm_provider = llm_provider
        self.llm = self._initialize_llm(llm_provider, model_name)
        self.system_prompt = self._get_system_prompt()
        self.rag_prompt = self._get_rag_prompt()

        logger.info(f"Response Generator initialized with {llm_provider} provider")

    def _initialize_llm(self, provider: str, model_name: Optional[str]):
        """Initialize LLM based on provider"""
        if provider == "openai" and OPENAI_AVAILABLE:
            api_key = os.getenv("OPENAI_API_KEY")
            if not api_key:
                logger.warning("OPENAI_API_KEY not set, using fallback")
                return None

            return ChatOpenAI(
                model=model_name or "gpt-4-turbo-preview",
                temperature=0.3,
                api_key=api_key
            )

        elif provider == "anthropic" and ANTHROPIC_AVAILABLE:
            api_key = os.getenv("ANTHROPIC_API_KEY")
            if not api_key:
                logger.warning("ANTHROPIC_API_KEY not set, using fallback")
                return None

            return ChatAnthropic(
                model=model_name or "claude-3-sonnet-20240229",
                temperature=0.3,
                api_key=api_key
            )

        else:
            logger.info("Using fallback rule-based system")
            return None

    def _get_system_prompt(self) -> str:
        """Get system prompt for agricultural agent"""
        return """Ø£Ù†Øª Ù…Ø³ØªØ´Ø§Ø± Ø²Ø±Ø§Ø¹ÙŠ Ø®Ø¨ÙŠØ± Ù…ØªØ®ØµØµ ÙÙŠ Ø§Ù„Ø²Ø±Ø§Ø¹Ø© Ø§Ù„Ø°ÙƒÙŠØ© ÙˆØ§Ù„Ø¯Ù‚ÙŠÙ‚Ø©. Ù…Ù‡Ù…ØªÙƒ Ù…Ø³Ø§Ø¹Ø¯Ø© Ø§Ù„Ù…Ø²Ø§Ø±Ø¹ÙŠÙ† ÙÙŠ:

1. **ØªØ­Ù„ÙŠÙ„ Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø­Ù‚ÙˆÙ„**: NDVIØŒ Ø±Ø·ÙˆØ¨Ø© Ø§Ù„ØªØ±Ø¨Ø©ØŒ Ø¯Ø±Ø¬Ø§Øª Ø§Ù„Ø­Ø±Ø§Ø±Ø©ØŒ Ø§Ù„Ø£Ù…Ø·Ø§Ø±
2. **ØªØ´Ø®ÙŠØµ Ø§Ù„Ù…Ø´Ø§ÙƒÙ„**: Ø§Ù„Ø¥Ø¬Ù‡Ø§Ø¯ Ø§Ù„Ù…Ø§Ø¦ÙŠØŒ Ù†Ù‚Øµ Ø§Ù„Ù…ØºØ°ÙŠØ§ØªØŒ Ø§Ù„Ø£Ù…Ø±Ø§Ø¶ØŒ Ø§Ù„Ø¢ÙØ§Øª
3. **ØªÙ‚Ø¯ÙŠÙ… ØªÙˆØµÙŠØ§Øª**: Ø§Ù„Ø±ÙŠØŒ Ø§Ù„ØªØ³Ù…ÙŠØ¯ØŒ Ø§Ù„Ù…ÙƒØ§ÙØ­Ø©ØŒ Ø§Ù„Ø¹Ù†Ø§ÙŠØ© Ø¨Ø§Ù„Ù…Ø­Ø§ØµÙŠÙ„
4. **Ø§Ù„ØªÙ†Ø¨Ø¤ ÙˆØ§Ù„ØªØ®Ø·ÙŠØ·**: ØªÙˆÙ‚Ø¹Ø§Øª Ø§Ù„Ù…Ø­ØµÙˆÙ„ØŒ Ø¥Ø¯Ø§Ø±Ø© Ø§Ù„Ù…Ø®Ø§Ø·Ø±ØŒ Ø§Ù„ØªØ®Ø·ÙŠØ· Ø§Ù„Ù…ÙˆØ³Ù…ÙŠ

**Ø¥Ø±Ø´Ø§Ø¯Ø§Øª Ø§Ù„Ø±Ø¯:**
- Ø§Ø³ØªØ®Ø¯Ù… Ø§Ù„Ù„ØºØ© Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© Ø§Ù„ÙØµØ­Ù‰ Ø§Ù„Ø¨Ø³ÙŠØ·Ø© ÙˆØ§Ù„ÙˆØ§Ø¶Ø­Ø©
- ÙƒÙ† Ù…Ø­Ø¯Ø¯Ø§Ù‹ ÙˆØ¯Ù‚ÙŠÙ‚Ø§Ù‹ ÙÙŠ Ø§Ù„ØªÙˆØµÙŠØ§Øª
- Ø§Ø¹ØªÙ…Ø¯ Ø¹Ù„Ù‰ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª ÙˆØ§Ù„Ù…Ø¹Ø±ÙØ© Ø§Ù„Ø¹Ù„Ù…ÙŠØ©
- Ù‚Ø¯Ù… Ø£ÙˆÙ„ÙˆÙŠØ§Øª ÙˆØ§Ø¶Ø­Ø© (Ø¹Ø§Ø¬Ù„ØŒ Ù…Ù‡Ù…ØŒ Ù…ØªØ§Ø¨Ø¹Ø©)
- Ø§Ø³ØªØ®Ø¯Ù… Ø§Ù„Ø±Ù…ÙˆØ² Ø§Ù„ØªØ¹Ø¨ÙŠØ±ÙŠØ© Ø¨Ø­ÙƒÙ…Ø© Ù„ØªÙˆØ¶ÙŠØ­ Ø§Ù„Ø£ÙˆÙ„ÙˆÙŠØ§Øª:
  * ğŸ”´ Ù„Ù„Ù…Ø´Ø§ÙƒÙ„ Ø§Ù„Ø­Ø±Ø¬Ø© Ø§Ù„ØªÙŠ ØªØ­ØªØ§Ø¬ ØªØ¯Ø®Ù„ ÙÙˆØ±ÙŠ
  * ğŸŸ¡ Ù„Ù„ØªØ­Ø°ÙŠØ±Ø§Øª Ø§Ù„Ù…Ù‡Ù…Ø©
  * ğŸŸ¢ Ù„Ù„Ø­Ø§Ù„Ø§Øª Ø§Ù„Ø¬ÙŠØ¯Ø©
  * ğŸ’§ Ù„Ù„Ø±ÙŠ
  * ğŸŒ± Ù„Ù„Ù†Ù…Ùˆ ÙˆØ§Ù„ØªØ·ÙˆØ±
  * ğŸ”¬ Ù„Ù„ØªØ­Ù„ÙŠÙ„Ø§Øª Ø§Ù„Ø¹Ù„Ù…ÙŠØ©

**Ù†Ù‡Ø¬Ùƒ:**
1. Ø­Ù„Ù„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…ØªÙˆÙØ±Ø© Ø¨Ø¯Ù‚Ø©
2. Ø§Ø±Ø¨Ø· Ø§Ù„Ù…Ø¤Ø´Ø±Ø§Øª Ø§Ù„Ù…Ø®ØªÙ„ÙØ© Ø¨Ø¨Ø¹Ø¶Ù‡Ø§
3. Ø§Ø³ØªØ®Ø¯Ù… Ø§Ù„Ù…Ø¹Ø±ÙØ© Ø§Ù„Ø²Ø±Ø§Ø¹ÙŠØ© Ù…Ù† Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
4. Ù‚Ø¯Ù… ØªÙˆØµÙŠØ§Øª Ø¹Ù…Ù„ÙŠØ© Ù‚Ø§Ø¨Ù„Ø© Ù„Ù„ØªÙ†ÙÙŠØ°
5. Ø§Ø´Ø±Ø­ Ø§Ù„Ø£Ø³Ø¨Ø§Ø¨ ÙˆØ§Ù„ØªÙˆÙ‚Ø¹Ø§Øª

ØªØ°ÙƒØ±: Ø£Ù†Øª ØªØ³Ø§Ø¹Ø¯ Ù…Ø²Ø§Ø±Ø¹ÙŠÙ† Ø­Ù‚ÙŠÙ‚ÙŠÙŠÙ†ØŒ Ù†ØµØ§Ø¦Ø­Ùƒ ØªØ¤Ø«Ø± Ø¹Ù„Ù‰ Ù…Ø¹ÙŠØ´ØªÙ‡Ù… ÙˆÙ…Ø­Ø§ØµÙŠÙ„Ù‡Ù….
"""

    def _get_rag_prompt(self) -> ChatPromptTemplate:
        """Get RAG prompt template"""
        template = """Ø§Ø³ØªØ®Ø¯Ù… Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„ØªØ§Ù„ÙŠØ© Ù…Ù† Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ù…Ø¹Ø±ÙØ© Ø§Ù„Ø²Ø±Ø§Ø¹ÙŠØ© Ù„Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø¹Ù„Ù‰ Ø§Ù„Ø³Ø¤Ø§Ù„:

**Ø§Ù„Ù…Ø¹Ø±ÙØ© Ø§Ù„Ø²Ø±Ø§Ø¹ÙŠØ© Ø°Ø§Øª Ø§Ù„ØµÙ„Ø©:**
{context}

**Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø­Ù‚Ù„ Ø§Ù„Ø­Ø§Ù„ÙŠØ©:**
{field_data}

**Ø§Ù„Ø³Ø¤Ø§Ù„/Ø§Ù„Ø·Ù„Ø¨:**
{question}

**ØªØ¹Ù„ÙŠÙ…Ø§Øª:**
1. Ø§Ø³ØªØ®Ø¯Ù… Ø§Ù„Ù…Ø¹Ø±ÙØ© Ø§Ù„Ù…ØªÙˆÙØ±Ø© Ø£Ø¹Ù„Ø§Ù‡ ÙƒÙ…Ø±Ø¬Ø¹ Ø£Ø³Ø§Ø³ÙŠ
2. Ø­Ù„Ù„ Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø­Ù‚Ù„ Ø¨Ø¯Ù‚Ø© ÙˆØ±Ø¨Ø·Ù‡Ø§ Ø¨Ø§Ù„Ù…Ø¹Ø±ÙØ© Ø§Ù„Ø²Ø±Ø§Ø¹ÙŠØ©
3. Ù‚Ø¯Ù… ØªÙˆØµÙŠØ§Øª Ù…Ø­Ø¯Ø¯Ø© ÙˆØ¹Ù…Ù„ÙŠØ© Ù…Ø¹ Ø§Ù„Ø£ÙˆÙ„ÙˆÙŠØ§Øª
4. Ø¥Ø°Ø§ ÙƒØ§Ù†Øª Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª ØºÙŠØ± ÙƒØ§ÙÙŠØ©ØŒ Ø§Ø°ÙƒØ± Ø°Ù„Ùƒ ÙˆØ§Ø·Ù„Ø¨ Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø¥Ø¶Ø§ÙÙŠØ©
5. ÙƒÙ† ÙˆØ§Ø¶Ø­Ø§Ù‹ ÙˆÙ…Ø¨Ø§Ø´Ø±Ø§Ù‹ ÙÙŠ Ø±Ø¯Ùƒ

Ø§Ù„Ø±Ø¯:"""

        return ChatPromptTemplate.from_messages([
            ("system", self.system_prompt),
            ("human", template),
        ])

    async def generate_with_llm(
        self,
        context: str,
        field_data: str,
        question: str
    ) -> str:
        """
        Generate response using LLM with RAG

        Args:
            context: Knowledge base context
            field_data: Formatted field data
            question: User question

        Returns:
            Generated response
        """
        if not self.llm:
            raise ValueError("LLM not initialized")

        try:
            # Build the chain
            chain = (
                {
                    "context": RunnableLambda(lambda _: context),
                    "field_data": RunnableLambda(lambda _: field_data),
                    "question": RunnablePassthrough()
                }
                | self.rag_prompt
                | self.llm
                | StrOutputParser()
            )

            # Invoke the chain
            response = await chain.ainvoke(question)
            return response

        except Exception as e:
            logger.error(f"LLM generation error: {e}", exc_info=True)
            raise

    def generate_rule_based(
        self,
        field_data: Dict[str, Any],
        context: str = ""
    ) -> str:
        """
        Generate response using rule-based system

        Args:
            field_data: Field data with metrics
            context: Optional knowledge context

        Returns:
            Rule-based analysis
        """
        warnings = []
        recommendations = []
        priority = "normal"

        # Soil analysis
        soil = field_data.get("soil_summary", {})
        if soil:
            ec = soil.get("ec_avg")
            ph = soil.get("ph_avg")
            moisture = soil.get("moisture_avg")

            if ec and ec > 4:
                warnings.append("ğŸ”´ **Ù…Ù„ÙˆØ­Ø© Ø§Ù„ØªØ±Ø¨Ø© Ù…Ø±ØªÙØ¹Ø© Ø¬Ø¯Ø§Ù‹** (EC > 4 dS/m)")
                recommendations.append(
                    "ğŸ’§ **Ø¹Ø§Ø¬Ù„**: Ù‚Ù… Ø¨ØºØ³Ù„ Ø§Ù„ØªØ±Ø¨Ø© Ø¨Ø±ÙŠ ØºØ²ÙŠØ± ÙˆØ­Ø³Ù‘Ù† Ø§Ù„ØµØ±Ù. "
                    "Ø£Ø¶Ù Ø§Ù„Ø¬Ø¨Ø³ Ø§Ù„Ø²Ø±Ø§Ø¹ÙŠ Ø¨Ù…Ø¹Ø¯Ù„ 2-3 Ø·Ù†/Ù‡ÙƒØªØ§Ø±."
                )
                priority = "high"
            elif ec and ec > 2:
                warnings.append("ğŸŸ¡ Ù…Ù„ÙˆØ­Ø© Ø§Ù„ØªØ±Ø¨Ø© Ù…ØªÙˆØ³Ø·Ø© (EC 2-4 dS/m)")
                recommendations.append(
                    "ğŸ’§ Ø±Ø§Ù‚Ø¨ Ø§Ù„Ù…Ù„ÙˆØ­Ø© Ø¹Ù† ÙƒØ«Ø¨. "
                    "ØªØ¬Ù†Ø¨ Ø§Ù„Ø£Ø³Ù…Ø¯Ø© Ø§Ù„Ù…Ù„Ø­ÙŠØ© ÙˆØ§Ø³ØªØ®Ø¯Ù… Ø§Ù„Ø±ÙŠ Ø¨Ø§Ù„ØªÙ†Ù‚ÙŠØ·."
                )

            if ph and (ph < 5.5 or ph > 7.5):
                warnings.append(
                    f"ğŸŸ¡ Ø¯Ø±Ø¬Ø© Ø­Ù…ÙˆØ¶Ø© Ø§Ù„ØªØ±Ø¨Ø© Ø®Ø§Ø±Ø¬ Ø§Ù„Ù†Ø·Ø§Ù‚ Ø§Ù„Ù…Ø«Ø§Ù„ÙŠ (pH: {ph:.1f})"
                )
                if ph < 5.5:
                    recommendations.append(
                        "ğŸ”¬ Ø£Ø¶Ù Ø§Ù„Ø¬ÙŠØ± Ø§Ù„Ø²Ø±Ø§Ø¹ÙŠ Ù„Ø±ÙØ¹ pH. "
                        "Ø§Ù„Ø¬Ø±Ø¹Ø© ØªØ¹ØªÙ…Ø¯ Ø¹Ù„Ù‰ Ù†ÙˆØ¹ Ø§Ù„ØªØ±Ø¨Ø© ÙˆØ§Ù„Ù…Ø­ØµÙˆÙ„."
                    )
                else:
                    recommendations.append(
                        "ğŸ”¬ Ø£Ø¶Ù ÙƒØ¨Ø±ÙŠØª Ø²Ø±Ø§Ø¹ÙŠ Ø£Ùˆ Ø£Ø³Ù…Ø¯Ø© Ø­Ù…Ø¶ÙŠØ© Ù„Ø®ÙØ¶ pH."
                    )

            if moisture and moisture < 20:
                warnings.append(f"ğŸ’§ **Ø±Ø·ÙˆØ¨Ø© Ø§Ù„ØªØ±Ø¨Ø© Ù…Ù†Ø®ÙØ¶Ø©** ({moisture:.1f}%)")
                recommendations.append(
                    "ğŸ’§ **Ù…Ù‡Ù…**: Ù‚Ù… Ø¨Ø§Ù„Ø±ÙŠ Ø®Ù„Ø§Ù„ 12-24 Ø³Ø§Ø¹Ø© Ø§Ù„Ù‚Ø§Ø¯Ù…Ø© Ù„ØªØ¬Ù†Ø¨ Ø§Ù„Ø¥Ø¬Ù‡Ø§Ø¯ Ø§Ù„Ù…Ø§Ø¦ÙŠ."
                )
                if priority != "high":
                    priority = "medium"

        # Weather analysis
        weather = field_data.get("weather_forecast", {})
        if weather:
            points = weather.get("points", [])
            if points:
                max_temp = max((p.get("temp_c") or 0) for p in points)
                total_rain = sum((p.get("rain_mm") or 0) for p in points)

                if max_temp > 40:
                    warnings.append(f"ğŸŒ¡ï¸ **Ø¯Ø±Ø¬Ø§Øª Ø­Ø±Ø§Ø±Ø© Ù…Ø±ØªÙØ¹Ø© Ù…ØªÙˆÙ‚Ø¹Ø©** ({max_temp:.0f}Â°Ù…)")
                    recommendations.append(
                        "ğŸŒ¡ï¸ Ø²Ø¯ Ù…Ø¹Ø¯Ù„ Ø§Ù„Ø±ÙŠ 20-30%. "
                        "ØªØ¬Ù†Ø¨ Ø§Ù„Ø±ÙŠ ÙÙŠ Ø³Ø§Ø¹Ø§Øª Ø§Ù„Ø°Ø±ÙˆØ© Ø§Ù„Ø­Ø±Ø§Ø±ÙŠØ© (12-4 Ù…Ø³Ø§Ø¡Ù‹)."
                    )

                if total_rain > 30:
                    warnings.append(f"ğŸŒ§ï¸ Ø£Ù…Ø·Ø§Ø± Ù…ØªÙˆÙ‚Ø¹Ø© ({total_rain:.0f} Ù…Ù…)")
                    recommendations.append(
                        "ğŸŒ§ï¸ Ù‚Ù„Ù„ Ø§Ù„Ø±ÙŠ Ø­Ø³Ø¨ ÙƒÙ…ÙŠØ© Ø§Ù„Ø£Ù…Ø·Ø§Ø±. "
                        "ØªØ£ÙƒØ¯ Ù…Ù† ÙƒÙØ§Ø¡Ø© Ø§Ù„ØµØ±Ù Ù„ØªØ¬Ù†Ø¨ ØªØ¬Ù…Ø¹ Ø§Ù„Ù…ÙŠØ§Ù‡."
                    )

        # NDVI analysis
        ndvi_data = field_data.get("imagery_latest", {})
        if ndvi_data:
            ndvi_avg = ndvi_data.get("ndvi_avg")
            if ndvi_avg and ndvi_avg < 0.4:
                warnings.append(f"ğŸ”´ **Ù…Ø¤Ø´Ø± NDVI Ù…Ù†Ø®ÙØ¶ Ø¬Ø¯Ø§Ù‹** ({ndvi_avg:.2f})")
                recommendations.append(
                    "ğŸŒ± **Ø¹Ø§Ø¬Ù„**: Ø§ÙØ­Øµ Ø§Ù„Ø­Ù‚Ù„ Ù…ÙŠØ¯Ø§Ù†ÙŠØ§Ù‹. "
                    "Ù‚Ø¯ ÙŠÙƒÙˆÙ† Ù‡Ù†Ø§Ùƒ Ø¥Ø¬Ù‡Ø§Ø¯ Ù…Ø§Ø¦ÙŠ Ø£Ùˆ Ù†Ù‚Øµ Ù…ØºØ°ÙŠØ§Øª Ø£Ùˆ Ù…Ø±Ø¶. "
                    "Ø§Ø®ØªØ¨Ø± Ø§Ù„ØªØ±Ø¨Ø© ÙˆØ±Ø§Ø¬Ø¹ Ø¨Ø±Ù†Ø§Ù…Ø¬ Ø§Ù„ØªØ³Ù…ÙŠØ¯."
                )
                priority = "high"
            elif ndvi_avg and ndvi_avg < 0.6:
                warnings.append(f"ğŸŸ¡ Ù…Ø¤Ø´Ø± NDVI Ø£Ù‚Ù„ Ù…Ù† Ø§Ù„Ù…Ø«Ø§Ù„ÙŠ ({ndvi_avg:.2f})")
                recommendations.append(
                    "ğŸŒ± Ø±Ø§Ù‚Ø¨ Ø§Ù„Ù†Ø¨Ø§ØªØ§Øª Ø¹Ù† ÙƒØ«Ø¨. "
                    "ØªØ­Ù‚Ù‚ Ù…Ù† Ø§Ù„Ø±ÙŠ ÙˆØ§Ù„ØªØ³Ù…ÙŠØ¯. Ù‚Ø¯ ØªØ­ØªØ§Ø¬ ØªØ³Ù…ÙŠØ¯ ÙˆØ±Ù‚ÙŠ Ù†ÙŠØªØ±ÙˆØ¬ÙŠÙ†ÙŠ."
                )

        # Build response
        response_parts = []

        if priority == "high":
            response_parts.append("## ğŸ”´ ØªÙ†Ø¨ÙŠÙ‡: Ø­Ø§Ù„Ø© Ø­Ø±Ø¬Ø© ØªØ­ØªØ§Ø¬ ØªØ¯Ø®Ù„ ÙÙˆØ±ÙŠ\n")
        elif priority == "medium":
            response_parts.append("## ğŸŸ¡ ØªØ­Ø°ÙŠØ±: ØªÙˆØ¬Ø¯ Ù…Ø¤Ø´Ø±Ø§Øª ØªØ­ØªØ§Ø¬ Ø§Ù‡ØªÙ…Ø§Ù…\n")
        else:
            response_parts.append("## ğŸŸ¢ Ø§Ù„Ø­Ø§Ù„Ø© Ø§Ù„Ø¹Ø§Ù…Ø© Ù…Ø³ØªÙ‚Ø±Ø©\n")

        if warnings:
            response_parts.append("### ğŸ“Š Ø§Ù„Ù…Ø¤Ø´Ø±Ø§Øª ÙˆØ§Ù„ØªØ­Ø°ÙŠØ±Ø§Øª:")
            for w in warnings:
                response_parts.append(f"- {w}")
            response_parts.append("")

        if recommendations:
            response_parts.append("### ğŸ“‹ Ø§Ù„ØªÙˆØµÙŠØ§Øª ÙˆØ§Ù„Ø¥Ø¬Ø±Ø§Ø¡Ø§Øª:")
            for i, r in enumerate(recommendations, 1):
                response_parts.append(f"{i}. {r}")
            response_parts.append("")

        if context and "Ù„Ø§ ØªÙˆØ¬Ø¯" not in context:
            response_parts.append("### ğŸ“š Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø¥Ø¶Ø§ÙÙŠØ© Ù…Ù† Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ù…Ø¹Ø±ÙØ©:")
            response_parts.append(context)
            response_parts.append("")

        if not warnings and not recommendations:
            response_parts.append(
                "âœ… **Ø§Ù„ÙˆØ¶Ø¹ Ø¬ÙŠØ¯**: Ø¬Ù…ÙŠØ¹ Ø§Ù„Ù…Ø¤Ø´Ø±Ø§Øª ÙÙŠ Ø§Ù„Ù†Ø·Ø§Ù‚ Ø§Ù„Ù…Ù‚Ø¨ÙˆÙ„. "
                "Ø§Ø³ØªÙ…Ø± Ø¨Ø§Ù„Ø¨Ø±Ù†Ø§Ù…Ø¬ Ø§Ù„Ø­Ø§Ù„ÙŠ Ù…Ø¹ Ø§Ù„Ù…ØªØ§Ø¨Ø¹Ø© Ø§Ù„Ø¯ÙˆØ±ÙŠØ©."
            )

        response_parts.append("\n---")
        response_parts.append(
            "ğŸ’¡ **Ù…Ù„Ø§Ø­Ø¸Ø©**: Ù‡Ø°Ø§ ØªØ­Ù„ÙŠÙ„ Ø¢Ù„ÙŠ Ù…Ø¨Ù†ÙŠ Ø¹Ù„Ù‰ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…ØªÙˆÙØ±Ø©. "
            "Ù„Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ù†ØµØ§Ø¦Ø­ Ø£ÙƒØ«Ø± Ø¯Ù‚Ø©ØŒ ÙŠÙÙ†ØµØ­ Ø¨ÙØ­Øµ Ù…ÙŠØ¯Ø§Ù†ÙŠ Ù…Ù† Ù…ØªØ®ØµØµ."
        )

        return "\n".join(response_parts)

    async def generate(
        self,
        context: str,
        field_data: str,
        field_data_dict: Dict[str, Any],
        question: str,
        use_llm: bool = True
    ) -> str:
        """
        Generate response (LLM or rule-based)

        Args:
            context: Knowledge context
            field_data: Formatted field data string
            field_data_dict: Raw field data dictionary
            question: User question
            use_llm: Whether to use LLM if available

        Returns:
            Generated response
        """
        if use_llm and self.llm:
            try:
                return await self.generate_with_llm(context, field_data, question)
            except Exception as e:
                logger.warning(f"LLM generation failed, using fallback: {e}")
                return self.generate_rule_based(field_data_dict, context)
        else:
            return self.generate_rule_based(field_data_dict, context)

    @property
    def is_llm_available(self) -> bool:
        """Check if LLM is available"""
        return self.llm is not None


# Global generator instance
_generator_instance: Optional[ResponseGenerator] = None


def get_generator(llm_provider: str = None, model_name: str = None) -> ResponseGenerator:
    """Get or create generator instance"""
    global _generator_instance

    if llm_provider is None:
        llm_provider = os.getenv("LLM_PROVIDER", "openai")

    if _generator_instance is None:
        _generator_instance = ResponseGenerator(llm_provider, model_name)

    return _generator_instance
